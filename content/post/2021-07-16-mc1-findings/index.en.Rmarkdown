---
title: 'MC1: Findings and Discoveries'
author: "Ng Yen Ngee"
date: '2021-07-24'
lastmod: '2021-07-25'
slug: []
cover: "/img/data_discovery.png"
categories: []
tags: ["MC1", 'MITB', 'Text Analytics']
output:
  blogdown::html_page: 
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3, 
                      echo=FALSE,
                      eval=TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Introduction 
In this post, I will be running through the findings and analysis of [Vast Challenge MC1](https://vast-challenge.github.io/2021/MC1.html). This will be my main page and answer to my class [assignment](https://isss608.netlify.app/assignment.html). 

The objective of this assignment is identify the complex relationships between the people, organizations, news articles given the data that we have. 


## Content of all Posts 

Below and the table of content on the left is how we can navigate through the posts and analysis. Each section has a full post dedicated which runs through explanations, codes and actual visualizations. As analyzing in R and the data set is new to me, some of the posts are documented in a step by step manner with a run through of how I managed to arrive at the last visualization for my analysis below. Hence, some of the posts are quite lengthy. If you want to skip to the analysis bit, please scroll down to the analysis section of this post. 

* Literature Review [Full Post](https://yenngee-dataviz.netlify.app/post/2021-07-17-mc1-literature-review/)
* Data Preparation [Full Post](https://yenngee-dataviz.netlify.app/post/2021-07-11-mc1-data-preperation/)
* Analysis 
  * 1a Primary Vs Derivative Sources [Full Post](https://yenngee-dataviz.netlify.app/post/2021-07-15-mc1-primary-vs-derivative-sources/)
  * 1b Relationships:Primary vs Derivative Sources [Full Post](https://yenngee-dataviz.netlify.app/post/2021-07-15-mc1-explore-relationships-between-articles/)
  * 2 Who is biased? [Full Post](https://yenngee-dataviz.netlify.app/post/2021-07-16-mc1-who-is-bias-to-whom/)
  * 3 Connections and Relationships [Full Post](https://yenngee-dataviz.netlify.app/post/2021-07-20-mc1-connections-revealed/)


## Literature Review 
This is not the first time that Vast Challenge has given this data sources and topic. The first time that Vast Challenge set this scenario is in 2014. Though the questions were completely different, there were many valuable information and methodologies that we could learn from. The complete **Literature Review** can be found [here](https://yenngee-dataviz.netlify.app/post/2021-07-17-mc1-literature-review/)


## Data Preperation
The complete **Data Preparation process** can be found [here](https://yenngee-dataviz.netlify.app/post/2021-07-11-mc1-data-preperation/). The data preparation has included a step by step description of understanding the data and transforming unstructured text such as the news articles into a structured dataframe. For this portion of the post, I will be starting from a cleaned data source. 

```{r load package}
library(tidyverse)
library(tidytext)
library(tidygraph)
library(stringr)
library(gridExtra)
library(lubridate)
library(ggraph)
library(igraph)
library(widyr)
library(visNetwork)
```

```{r load data}
cleaned_text_source <- read_rds("data/news_article_clean_primary_vs_derivative.rds")
```

## Analysis
### 1a) Primary Vs Derivative Sources 
This section will answer part 1a of the challenge: Which are primary sources and which are derivative sources? The complete step by step thought process and documentation can be found [here](https://yenngee-dataviz.netlify.app/post/2021-07-15-mc1-primary-vs-derivative-sources/)

Primary Sources is "first-hand" information, while secondary sources provide analysis, commentary or criticism on the Primary source. In terms of data, we've defined primary source as the following: 
- article text start with a time format 
- article title contains the word blog 
- duplicated article titles that is printed first 

We have the following output: 

```{r source_type_per_article}
full_source_type_per_article <- expand_grid(source = unique(cleaned_text_source$source), 
                                             is_primary = c('primary', 'derivative')) 

source_type_per_article <- cleaned_text_source %>%
  group_by(source, is_primary, primary_type) %>%
  summarize(n = n()) %>% 
  ungroup() %>%
  right_join(full_source_type_per_article, by = c('source', 'is_primary')) %>%
  arrange(desc(n))

source_type_per_article_perc <- cleaned_text_source %>%
  group_by(source, is_primary) %>%
  summarize(n = n()) %>% 
  ungroup() %>%
  right_join(full_source_type_per_article, by = c('source', 'is_primary')) %>%
  arrange(desc(n)) %>% 
  pivot_wider(names_from= is_primary, values_from=n) %>%
  replace_na(list(derivative=0L, primary=0L)) %>%
  mutate(primary_perc = primary/(derivative + primary), 
         source = as.character(source)) %>%
  select(source, primary_perc)

source_type_per_article <- source_type_per_article %>%
  left_join(source_type_per_article_perc) %>%
  mutate(source = fct_reorder(source, primary_perc))
```

**Primary VS Derivative of each source**

```{r plot_pyramid3, echo=FALSE}
plot_derivative <- source_type_per_article %>% 
  filter(is_primary == "derivative") %>%
  ggplot(aes(x=source, y=n, fill=primary_type)) +
  geom_bar(stat='identity', fill="#0072B2") +
  scale_y_continuous('') + 
  ylim(0,120)+
  theme(legend.position = 'none',
        axis.title.y = element_blank(),
        plot.title = element_text(size = 11.5, hjust = 0.5),
        plot.margin=unit(c(0.1,0.2,0.1,-.1),"cm"),
        axis.ticks.y = element_blank(), 
        axis.text.y = element_text(hjust = 0.5)) + #theme_bw()$axis.text.y,
  ggtitle("Derivative") + 
  coord_flip() 
  
plot_primary <- source_type_per_article %>% 
  filter(is_primary == "primary") %>%
  ggplot(aes(x=source, y=n)) +
  geom_bar(stat='identity', fill="#D55E00") +
  # ylim(0,120)+
  scale_y_continuous(trans = 'reverse', limits=c(120,0))+ 
  theme(legend.position = 'none',
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(), 
        plot.title = element_text(size = 11.5, hjust = 0.5),
        plot.margin=unit(c(0.1,0,0.1,0.05),"cm")) + 
  ggtitle("Primary") +
  coord_flip()

plot_sources <- source_type_per_article %>% 
  filter(is_primary == "derivative") %>%
  ggplot(aes(x=source, y=n)) +
  geom_bar(stat='identity')+
  geom_text(aes(y=0, label=source))+ 
  coord_flip()

grid.arrange(plot_primary, plot_derivative, 
             widths=c(0.4,0.6),
             ncol=2
)
``` 


**Percentile of Primary VS Derivative of each source**

```{r plot_perc_bar, echo=FALSE}
source_type_per_article %>%
  ggplot(aes(fill=is_primary, y=n, x=source)) +
  geom_bar(position="fill", stat="identity") +
  coord_flip()

```

```{r plot_breakdown_primary_type}
source_type_per_article %>%
  filter(primary_type %in% c('first post', 'blog post')) %>%
  ggplot(aes(fill=primary_type, y=n, x=source)) +
  geom_bar( stat="identity") +
  coord_flip()
```

The three visualizations shows the breakdown of primary vs derivative sources and the type of primary source that we have managed to identify. 

We can see that "Modern Rubicon" and "Centrum Sentinel" are fully blog posts, while "Homeland illumination" and "Abila Post", has a mix of blog posts and being the first to post. This is interesting to note as later on in part 1a) we can see that some of the news sources actually "copy" the information, almost like a loudhailer for the particular news article source. 

For simplicity, we defined primary type as primary if >75% of the articles are considered primary, else if between 25%-75% then it is considered partially derivative and the rest are derivative. 

```{r news_article_groups}
news_article_groups <- tribble(
  ~source, ~group, ~is_primary, ~bias, 
  "The Light of Truth", "A", 'Derivative', 'gastech',
  "The General Post",   "A", 'Derivative', 'gastech',
  "Who What News",      "A", 'Derivative', 'gastech',
  "The Tulip",          "A", 'Derivative', 'gastech',
  "The World",          "A", 'Primary', 'gastech',
  "World Journal",      "B", 'Derivative', 'unknown',
  "Everyday News",      "B", 'Derivative', 'unknown',
  "The Continent",      "B", 'Derivative', 'unknown',
  "World Source",       "B", 'Partially Primary', 'unknown',
  "International Times","B", 'Partially Primary', 'unknown',
  "All News Today",     "C", 'Partially Primary', 'unknown',
  "The Wrap",           "C", 'Derivative', 'unknown',
  "Daily Pegasus",      "C", 'Derivative', 'unknown',
  "The Orb",            "C", 'Derivative', 'unknown',
  "Homeland Illumination", "C", 'Primary', 'pok',
  "International News", "D", 'Derivative', 'unknown',
  "Worldwise",          "D", 'Derivative', 'unknown',
  "The Truth",          "D", 'Derivative', 'unknown',
  "The Guide",          "D", 'Derivative', 'unknown',
  "Kronos Star",        "D", 'Partially Primary', 'unknown',
  "News Desk",          "E", 'Derivative', 'unknown',
  "The Explainer",      "E", 'Derivative', 'unknown',
  "Athena Speaks",      "E", 'Derivative', 'unknown',
  "Central Bulletin",   "E", 'Derivative', 'gastech',
  "The Abila Post",     "E", 'Primary', 'unknown',
  "News Online Today",  "Others", 'Derivative', 'unknown',
  "Modern Rubicon",     "Others", 'Primary', 'gastech',
  "Centrum Sentinel",   "Others", 'Primary', 'gastech',
  "Tethys News",        "Others",  'Derivative','gastech' 
)

news_article_groups %>% select(source, is_primary)
```

### 1b) Relationships:Primary vs Derivative Sources
This section will answer part 1b of the challenge: What are the relationships between the primary and derivative sources? The complete step by step thought process and documentation can be found [here](https://yenngee-dataviz.netlify.app/post/2021-07-15-mc1-explore-relationships-between-articles/)

We started by tokenizing the article data that we have. With that, we are able to find out, based on the single words, which of the new articles are most similar using the `pairwise_cor` function. Below we have the network graph 

**How similar are the news articles?** 
```{r cor_monogram}
custom_stop_words <- tribble(
  ~word, ~lexicon,
  "kronos", "CUSTOM",
  "abila",  "CUSTOM"
)
stop_words2 <- stop_words %>%
  bind_rows(custom_stop_words)

token_words <- cleaned_text_source %>% 
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"), # only keep words. exclude all numeric. 
         !word %in% stop_words2$word) # to remove stop words 

words_by_articles <- token_words %>% 
  count(source, word, sort=TRUE) %>% 
  ungroup()

articles_cors <- words_by_articles %>%
  pairwise_cor(source, word,n,sort=TRUE) %>% 
  distinct(correlation, .keep_all = TRUE)

articles_cors %>%
  filter(correlation > .65) %>% 
  graph_from_data_frame() %>%
  ggraph(layout='fr') + 
  geom_edge_link(aes(alpha = correlation) , width = 1.5) + #alpha gives the shade
  geom_node_point(size=6, color="lightblue") +
  geom_node_text(aes(label=name), color="red", repel=TRUE) + 
  theme_void()
```

We notice that they news articles are naturally split into groups of 5, with a few exceptions. This is further supported by looking at articles with duplicated titles. Some of the articles posts before others. The lag in the date of post tells us which of the articles 'copies' others. The arrows point to the article source which reported the particular article first.


**Which article is coping who?** 
```{r dup_articles}
article_titles_dup <- cleaned_text_source %>%
  group_by(title) %>%
  add_count(source) %>% 
  summarize(n_distinct_source = n_distinct(source), 
            n_distinct_date = n_distinct(published_date), 
            min_date = min(published_date),
            n = n()) %>%
  ungroup() %>%
  filter(n>1 & n_distinct_source>1 & n_distinct_date > 1) %>%
  arrange(desc(n))

dup_articles <- cleaned_text_source %>%
  filter(title %in% unique(article_titles_dup$title)) %>%
  arrange(desc(title), desc(source)) %>%
  left_join(article_titles_dup %>% select(title, min_date)) %>%
  drop_na(published_date, min_date) %>%
  mutate(days_lagged = published_date - min_date) %>%
  mutate(days_lagged = ifelse(days_lagged >=2, '>=2 days', paste(days_lagged, 'day'))) 

lag_nodes <- tibble(unique(cleaned_text_source$source), .name_repair = ~c("label")) %>%
  rowid_to_column('id') %>%
  left_join(news_article_groups, by = c("label" = "source")) %>%
  rename(cluster = group, group = is_primary)

no_lag <- dup_articles %>%
  filter(days_lagged == "0 day") %>%
  select(source, title)

with_lag <- dup_articles %>%
  filter(days_lagged != "0 day") %>%
  select(source, title, days_lagged)

lag_edges <- right_join(no_lag, with_lag, by= "title") %>%
  rename(source_label = source.x,
         target_label = source.y) %>%
  left_join(lag_nodes %>% select(label, id), by = c("source_label" = "label")) %>%
  rename(from = id) %>%
  left_join(lag_nodes %>% select(label, id), by = c("target_label" = "label")) %>%
  rename(to = id)

lag_edges_agg <- lag_edges %>%
  mutate(arrows = "from") %>%
  group_by(from, to, source_label, target_label, arrows) %>%
  summarize(value = n()) %>%
  ungroup() %>%
  mutate(smooth = TRUE) %>%
  arrange(desc(value))

visNetwork(lag_nodes %>% filter(id %in% c(lag_edges_agg$from, lag_edges_agg$to)), lag_edges_agg) %>%
  visIgraphLayout(layout = "layout_with_fr")%>%
  visOptions(highlightNearest=TRUE,
             nodesIdSelection=TRUE)%>%
  visLegend()%>%
  visLayout(randomSeed=123)
```

We can see that "News Online Today" actually grab most of their news from the 3 primary sources and even some of the partially primary source from weight of the arrow. What is interesting is that we find articles also posting after News Online Today though with a smaller degree. All of the primary sources have articles referring to them. What is interesting is that "Centrum Sentinel" and "Modern Rubicon" though are considered primary sources, do not have any duplicated titled articles with other news articles. 


### 2) Who is biased? 
This section will answer part 2 of the challenge: Characterize any biases you identify in these news sources, with respect to their representation of specific people, places, and events. The complete step by step thought process and documentation can be found [here](https://yenngee-dataviz.netlify.app/post/2021-07-16-mc1-who-is-bias-to-whom/)

**How many times do pok/governement/gastech appear?** 
```{r freq_of_org_name}
freq_of_org <- token_words %>%
  filter(word %in% c("pok", "government", "gastech")) %>%
  group_by(source) %>% 
  summarize(total = n()) %>%
  ungroup()

perc_of_org <- token_words %>%
  filter(word %in% c("pok", "government", "gastech")) %>%
  group_by(source, word) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  left_join(freq_of_org) %>% 
  mutate(perc = n/total, 
         source=fct_reorder(source,perc))

perc_of_org %>%
  ggplot(aes(x=source, y = n, fill=word)) +
  geom_bar(stat='identity') + 
  coord_flip() +
  facet_wrap(~word)
```


**What is the proportion of pok/governement/gastech in news articles?**
```{r freq_of_org_name_perc}
perc_of_org %>%
  ggplot(aes(x=source, y = perc, fill=word)) +
  geom_bar(stat='identity') + 
  coord_flip()
```

We can see immediately that there are 8 news sources that do not or rarely mention pok, we can infer that these articles write mainly about gastech. 

* The World
* The Tulip
* The General Post
* The Light of Truth
* Who What News
* Centrum Sentinel 
* Modern Rubicon 
* Tethys News 

However, for the other news articles, they generally have an even distribution of mentions of pok, government and gastech. From our literature review, we understand that in particular "Homeland Illumination" has a strong inclination for POK.  

Thus we ran a sentiment analysis to see what are the sentiments of each news article. 

```{r sentiment_analysis_pok}
token_sentiments <- token_words %>% 
  left_join(get_sentiments("bing"))

org_words <- token_words %>%
  filter(word %in% c("pok"))

org_count_words <- token_words %>% 
  filter(article_id %in% unique(org_words$article_id)) %>% 
  count(word, sort=TRUE) %>%
  arrange(desc(n))

sentiment_by_grp <- token_sentiments %>%
  filter(sentiment %in% c("positive", "negative") & article_id %in% unique(org_words$article_id)) %>%
  count(source, sentiment) %>%
  spread(sentiment,n) %>% 
  mutate(overall_sentiment = positive-negative, 
         source = fct_reorder(source, overall_sentiment))

sentiment_by_grp %>% 
  ggplot(aes(x = source, y = overall_sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Article with articles that contains POK",
    subtitle = "Words in articles",
    x = "Article",
    y = "Overall Sentiment"
    )
```
```{r sentiment_analysis_gastech}
token_sentiments <- token_words %>% 
  left_join(get_sentiments("bing"))

org_words <- token_words %>%
  filter(word %in% c("gastech"))

org_count_words <- token_words %>% 
  filter(article_id %in% unique(org_words$article_id)) %>% 
  count(word, sort=TRUE) %>%
  arrange(desc(n))

sentiment_by_grp <- token_sentiments %>%
  filter(sentiment %in% c("positive", "negative") & article_id %in% unique(org_words$article_id)) %>%
  count(source, sentiment) %>%
  spread(sentiment,n) %>% 
  mutate(overall_sentiment = positive-negative, 
         source = fct_reorder(source, overall_sentiment))

sentiment_by_grp %>% 
  ggplot(aes(x = source, y = overall_sentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Article with articles that contains GASTECH",
    subtitle = "Words in articles",
    x = "Article",
    y = "Overall Sentiment"
    )
```

We can observe that even though "Homeland Illumination" is pro-POK, the general tone of the article is still very negative whether or not the article contains the words POK or GASTech. This suggests that the tone of these articles slants towards POK by painting a negative situation. From this we can see that "The Truth" and "The Guide" are probably pro-GASTech as well.

On a side note: The Abila Post overall contains many very negative news articles. 

### 3) Connections and Relationships 
This section will answer part 3 of the challenge: Given the data sources provided, use visual analytics to identify potential official and unofficial relationships among GASTech, POK, the APA, and Government. The complete step by step thought process and documentation can be found [here](https://yenngee-dataviz.netlify.app/post/2021-07-20-mc1-connections-revealed/) 

From some literature review and assumptions that those with the same surname are family, we can see the connections below. 

```{r people_network_viz}

# Nodes 
people_nodes <- read_rds("data/people_nodes.rds") %>% 
  rename(label = name)

#Edges

people_edges <- read_rds("data/people_edges.rds") %>%
    mutate(smooth = TRUE) # Additional terms for the visualization

visNetwork(people_nodes %>% filter(id %in% c(people_edges$from, people_edges$to)), people_edges) %>%
  visIgraphLayout(layout = "layout_with_fr")%>%
  visOptions(highlightNearest=TRUE, 
             nodesIdSelection=TRUE)%>%
  visLayout(randomSeed=123)
```

We observe that there are many 'family' members within GASTech. However, what is concerning are the relationships between POK memebers and GASTech members: 
* 

#### View connection via email headers 

We can view the connection between employees within GASTech by selecting the relavent email headers. 

<iframe src="https://yenngee-dataviz.shinyapps.io/email_headers/" width=700 height=500"></iframe>


#### Highlights of Suspicious Emails sent by Suspicious People

**Action: Virus detected on your system**
```{r, echo=FALSE}
knitr::include_graphics("image/Email_action_virus_detected_on_your_system.JPG")
```

**Cute**
```{r, echo=FALSE}
knitr::include_graphics("image/Email_cute.JPG")
```

**Files**
```{r, echo=FALSE}
knitr::include_graphics("image/Email_files.JPG")
```

**FW: Arise inspiration for defenders of Kronos**
```{r, echo=FALSE}
knitr::include_graphics("image/Email_fw_arise_inspiration_for_defenders_of_kronos.JPG")
```

**Plants**
```{r, echo=FALSE}
knitr::include_graphics("image/Email_plants.JPG")
```

**Do you like the folowers**
```{r, echo=FALSE}
knitr::include_graphics("image/Email_do_you_like_the_flowers.JPG")
```