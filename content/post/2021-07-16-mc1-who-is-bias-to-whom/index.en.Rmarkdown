---
title: 'MC1: Who is bias to whom?'
author: 'Ng Yen Ngee'
date: '2021-07-19'
lastmod: '2021-07-25'
slug: []
cover: "/img/bias.jpg"
categories: []
tags: ['MITB', 'Text Analytics', "MC1"]
output:
  blogdown::html_page: 
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3, 
                      echo=TRUE,
                      eval=TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Introduction 
In this post, I will be running through the thought process figuring out the biasness of the news article sources from [Vast Challenge MC1](https://vast-challenge.github.io/2021/MC1.html)

The final analysis done can be found [here](https://yenngee-dataviz.netlify.app/post/2021-07-16-mc1-findings/).

## Preperation 
### Load Packages 
These are the packages used for this post. 
```{r load package}
library(tidyverse)
library(tidytext)
library(stringr)
library(gridExtra)
library(lubridate)
library(wordcloud)
library(tidygraph)
library(igraph)      
library(ggraph)
```

### Load Data 
The data has been previously loaded, cleaned and transformed into a neat tibble dataframe [here](https://yenngee-dataviz.netlify.app/post/2021-07-11-mc1-data-preperation/). 
We load the cleaned data directly below: 

```{r load_clean_text}
cleaned_text <- read_rds("data/news_article_clean.rds")
cleaned_text <- cleaned_text %>%
  mutate(title = tolower(title))
glimpse(cleaned_text)
```

### Tokenize Data
The process of the tokenizing the data is written in detail [here](https://yenngee-dataviz.netlify.app/post/2021-07-11-mc1-data-preperation/#tokenising). This is how the `token_words` look like. 

```{r token_stopwords, echo=FALSE}
custom_stop_words <- tribble(
  ~word, ~lexicon,
  "kronos", "CUSTOM",
  "abila",  "CUSTOM"
)
stop_words2 <- stop_words %>%
  bind_rows(custom_stop_words)

token_words <- cleaned_text %>% 
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"), # only keep words. exclude all numeric. 
         !word %in% stop_words2$word) # to remove stop words 
```

```{r token_words}
glimpse(token_words)
```

## Analysis 
### Frequency 
We want to pick out certain words and see for each articles mentions it the highest. 

```{r freq_of_pok}
token_words %>%
  filter(word == "pok") %>%
  group_by(source) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  mutate(source=fct_reorder(source,n)) %>%
  ggplot(aes(x=source, y = n)) +
  geom_bar(stat='identity') + 
  coord_flip()
```
We see Kronos Star mention pok the most, while Centrum Sentinel mentions the least. 

Let's see similar graph but including other words. 


```{r freq_of_org}
freq_of_org <- token_words %>%
  filter(word %in% c("pok", "government", "gastech")) %>%
  group_by(source) %>% 
  summarize(total = n()) %>%
  ungroup()

perc_of_org <- token_words %>%
  filter(word %in% c("pok", "government", "gastech")) %>%
  group_by(source, word) %>% 
  summarize(n = n()) %>%
  ungroup() %>% 
  left_join(freq_of_org) %>% 
  mutate(perc = n/total, 
         source=fct_reorder(source,perc))

perc_of_org %>%
  ggplot(aes(x=source, y = n, fill=word)) +
  geom_bar(stat='identity') + 
  coord_flip() +
  facet_wrap(~word)

perc_of_org %>%
  ggplot(aes(x=source, y = perc, fill=word)) +
  geom_bar(stat='identity') + 
  coord_flip() +
  facet_wrap(~word)
```

### words association

```{r words_associated_with_pok}
pok_words <- token_words %>%
  filter(word %in% c("pok"))

pok_count_words <- token_words %>% 
  filter(article_id %in% unique(pok_words$article_id) & !word == "pok") %>% 
  count(word, sort=TRUE) %>%
  mutate(word = fct_reorder(word, n))

pok_count_words %>%
  top_n(15) %>%
  ggplot(aes(x= word, y=n)) + 
  geom_col() +
  coord_flip()

wordcloud(words = pok_count_words$word, 
          freq = pok_count_words$n, 
          max.words = 100)
```
```{r words_associated_with_org}
pok_words <- token_words %>%
  filter(word %in% c("pok", 'government', 'gastech'))

pok_count_words <- token_words %>% 
  filter(article_id %in% unique(pok_words$article_id)) %>% 
  count(word, sort=TRUE) %>%
  arrange(desc(n))

pok_count_words
```

### sentinent analysis 

```{r add_sentiment}
token_sentiments <- token_words %>% 
  left_join(get_sentiments("bing"))

sentiment_by_grp <- token_sentiments %>%
  filter(sentiment %in% c("positive", "negative") & article_id %in% unique(pok_words$article_id)) %>%
  count(source, sentiment) %>%
  spread(sentiment,n) %>% 
  mutate(overall_sentiment = positive-negative, 
         source = fct_reorder(source, overall_sentiment))

sentiment_by_grp %>% 
  ggplot(aes(x = source, y = overall_sentiment, fill = as.factor(source))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Overall Sentiment by Article with articles that contains POK",
    subtitle = "Words in articles",
    x = "Article",
    y = "Overall Sentiment"
    )

``` 

### Network Analysis 
Let's try this. 
The source nodes are news article source and the target nodes are the words. 

```{r nodes_edges_data}
# creating nodes df
source_tibble <- as_tibble(unique(token_words$source), column_name = "label") %>% 
  mutate(type = 'news article')%>% 
  rename('label' = 'value')
word_tibble <- as_tibble(unique(token_words$word)) %>% 
  mutate(type = 'word') %>% 
  rename('label' = 'value')
article_nodes <- source_tibble %>% 
  bind_rows(word_tibble) %>% 
  rowid_to_column("id")

# create edges df
article_edges <- token_sentiments %>%
  rename(target_label = word, source_label = source) %>%
  left_join(article_nodes %>% select(label, id), by = c("source_label" = "label")) %>% 
  rename(source = id) %>%
  left_join(article_nodes %>% select(label, id), by = c("target_label" = "label")) %>% 
  rename(target = id) 

article_edges_agg <- article_edges %>% 
  group_by(source, target, source_label, target_label) %>%
  summarize(weight = n()) %>%
  ungroup() %>% 
  filter(weight > 20)

article_graph <- tbl_graph(nodes = article_nodes, 
                          edges = article_edges_agg, 
                          directed = TRUE)

article_edges_agg %>%
  arrange(desc(weight))
```

As of today, the graph do not work. For future work and considerations: 

```{r network}
# ggraph(article_graph) + 
#   geom_edge_link() +
#   geom_node_point()

```