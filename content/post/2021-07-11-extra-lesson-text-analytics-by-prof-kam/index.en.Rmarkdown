---
title: "Lesson 10 Text Analytics by Prof Kam"
author: "Ng Yen Ngee"
date: '2021-07-11'
slug: []
cover: "/img/text_analytics2.jpg"
categories: []
tags: ['MITB', 'Text Analytics']
output:
  blogdown::html_page: 
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3, 
                      echo=TRUE,
                      eval=TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Introduction to in-class exercise 10
As part of my lesson in SMU Visual Analytics, prof teaches data visualization in R using Rmarkdown. The post below acts as both an in-class exercise for the lesson, as well as my notes. 

Note to prof, if he is here: I have reorganized the in class exercise in a way that makes sense to me, but may not follow the step by step process prof went through in class. I have added additional items which I have kept as haphazard notes on my desktop.  Hope this is alright! 


## Preperation 

### Loading packages

Our very first step of course is to load the packages that would be useful for us. 

```{r load package, echo=TRUE, results='hide'}
library(tidyverse)
library(tidytext)
library(widyr)
library(wordcloud)
library(ggwordcloud)
library(textdata)
library(textplot)
library(DT)
library(tidygraph)
library(ggraph)
library(igraph)
#time
library(lubridate)
# library(hms)
```


### Load data 

```{r read_files_fn, eval=FALSE}
news20 <- "data/20news/"

read_folder <- function(infolder) {
  tibble(file=dir(infolder, 
                  full.names=TRUE)) %>% 
    mutate(text = map(file, read_lines)) %>%
    transmute(id = basename(file), text) %>%
    unnest(text)
}

# how the function works 
# alt.atheism_df <- read_folder("data/20news/alt.atheism")
```

```{r read_files}
# raw_text <- tibble(folder = dir(news20, full.names=TRUE)) %>%
#   mutate(folder_out = map(folder, read_folder)) %>%
#   unnest(cols = c(folder_out)) %>%
#   transmute(newsgroup = basename(folder), id, text)
# 
# write_rds(raw_text, "data/rds/news20.rds")

raw_text <- read_rds("data/rds/news20.rds")
glimpse(raw_text)

```

To check if there is the correct number of 

```{r initial_eda}
raw_text %>%
  group_by(newsgroup) %>%
  summarize(messages=n_distinct(id)) %>%
  ggplot(aes(messages, newsgroup)) +
  geom_col(fill="lightblue") + 
  labs(y=NULL)
```

### Clean Data 

```{r clean_data}
cleaned_text <- raw_text %>%
  group_by(newsgroup, id) %>%
  filter(cumsum(text == "") >0, 
         cumsum(str_detect(text, "^--")) == 0) %>%
  ungroup()

cleaned_text 

cleaned_text <- cleaned_text %>% 
  filter(str_detect(text, "^[^>]+[A-Za-z\\d]") | text == "", 
         !str_detect(text, "writes(:|\\.\\.\\.)$"),
         !str_detect(text, "^In article <"))

cleaned_text 
```

## Get a bag of words 

### Tokenize 
```{r token no stopwords}
# usenet_words_own <- cleaned_text %>% 
#   unnest_tokens(word, text) %>%
#   anti_join(stop_words) # to remove stop words 
# 
# usenet_words_own

usenet_words <- cleaned_text %>% 
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$"), # only keep words. exclude all numeric. 
         !word %in% stop_words$word) # to remove stop words 

usenet_words 
```

### visualize 

```{r agg_words}

usenet_words %>% 
  count(word, sort=TRUE)

words_by_newsgroup <- usenet_words %>% 
  count(newsgroup, word, sort=TRUE) %>% 
  ungroup()

words_by_newsgroup
```

To visualize the words, we can use word cloud 

```{r wordcloud}
wordcloud(words = words_by_newsgroup$word, 
          freq = words_by_newsgroup$n, 
          max.words = 50)
```

```{r ggwordcloud}
set.seed(1234)

words_by_newsgroup %>%
  filter(n>5) %>%
  ggplot(aes(label=word, size = n)) + 
  geom_text_wordcloud() + 
  theme_minimal() + 
  facet_wrap(~newsgroup)

```


## TF-IDF

### create tfidf dataframe
```{r tfidf}
tf_idf <- words_by_newsgroup %>% 
  bind_tf_idf(word, newsgroup, n) %>%
  arrange(desc(tf_idf))

glimpse(tf_idf)
```

### visualize tfidf 

```{r tfidf_viz}
tf_idf %>%
  filter(str_detect(newsgroup, "^sci\\.")) %>%
  group_by(newsgroup) %>%
  slice_max(tf_idf, n=12) %>%
  ungroup() %>%
  mutate(word = reorder(word,tf_idf)) %>%
  ggplot(aes(tf_idf, word, fill=newsgroup)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~newsgroup, scales = "free") + 
  labs(x="tf-idf", 
       y=NULL)
```

## find correlation between groups
we want to know which newsgroup is more similar based on the words they use. 

```{r widyr}
newsgroup_cors <- words_by_newsgroup %>%
  pairwise_cor(newsgroup, word,n,sort=TRUE)

newsgroup_cors
```

```{r}
newsgroup_cors %>%
  filter(correlation > .025) %>%
  graph_from_data_frame() %>%
  ggraph(layout='fr') + 
  geom_edge_link(aes(alpha = correlation, width = correlation)) +
  geom_node_point(size=6, color="lightblue") +
  geom_node_text(aes(label=name), color="red", repel=TRUE) + 
  theme_void()
  

```

## ngrams
We look at words as consecutive terms. 

### bigrams 

```{r bigrams}
bigrams <- cleaned_text %>%
  unnest_tokens(bigram, text, token = "ngrams", n=2)

bigrams_sep <- bigrams %>% 
  filter(bigram != 'NA') %>%
  separate(bigram, c("word1", "word2"), sep=" ")

bigrams_filtered <- bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2%in% stop_words$word)

bigrams_filtered
```
```{r bigram_viz}
bigram_counts <- bigrams_filtered %>%
  mutate(word = paste(word1, word2, sep=" ")) %>%
  count(word, sort=TRUE) %>%
  filter(n>3) 
bigram_counts

bigram_counts %>%
  graph_from_data_frame() %>%
  ggraph(layout='fr') + 
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label=name), vjust=1, hjust=1) + 
  theme_void()

a <- grid::arrow(type="closed", 
                 length=unit(.15, "inches"))
bigram_counts %>%
  graph_from_data_frame() %>%
  ggraph(layout='fr') + 
  geom_edge_link( #aes(edge_alpha = n), 
                 show.legend = FALSE, 
                 arrow = a, 
                 end_cap = circle(.07, 'inches')
                 ) +
  geom_node_point(color = "lightblue", 
                  size=5) +
  geom_node_text(aes(label=name), vjust=1, hjust=1) + 
  theme_void()
```